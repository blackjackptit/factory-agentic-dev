#!/usr/bin/env python3
"""
AWS Batch Executor Script
Runs inside AWS Batch container to execute individual tasks.
Reads task definition from S3, executes, and writes result back to S3.
"""

import os
import sys
import json
import time
import subprocess
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any


def s3_download(s3_path: str, local_path: str, region: str) -> bool:
    """Download file from S3"""
    try:
        result = subprocess.run(
            ["aws", "s3", "cp", s3_path, local_path, "--region", region],
            capture_output=True,
            text=True,
            timeout=60
        )
        return result.returncode == 0
    except Exception as e:
        print(f"S3 download failed: {e}")
        return False


def s3_upload(local_path: str, s3_path: str, region: str) -> bool:
    """Upload file to S3"""
    try:
        result = subprocess.run(
            ["aws", "s3", "cp", local_path, s3_path, "--region", region],
            capture_output=True,
            text=True,
            timeout=60
        )
        return result.returncode == 0
    except Exception as e:
        print(f"S3 upload failed: {e}")
        return False


def execute_task_simulated(task: Dict[str, Any], output_dir: Path, executor_id: int) -> Dict[str, Any]:
    """Execute task in simulated mode (for testing)"""
    task_output_dir = output_dir / f"executor_{executor_id}" / task["id"]
    task_output_dir.mkdir(parents=True, exist_ok=True)

    start_time = time.time()

    # Simulate work
    steps = 5
    for step in range(1, steps + 1):
        time.sleep(1)
        progress = (step / steps) * 100
        print(f"  Progress: {progress:.0f}% - Step {step}/{steps}")

    # Create simulated output files
    output_files = []
    for filename in ["implementation.py", "tests.py", "README.md"]:
        file_path = task_output_dir / filename
        with open(file_path, "w") as f:
            f.write(f"# {task['name']}\n\n")
            f.write(f"# Generated by AWS Batch Executor {executor_id}\n")
            f.write(f"# Task ID: {task['id']}\n")
            f.write(f"# Task: {task['description']}\n\n")
            f.write(f"# Implementation code would go here\n")
        output_files.append(str(file_path))

    execution_time = time.time() - start_time

    return {
        "task_id": task["id"],
        "executor_id": executor_id,
        "task_name": task["name"],
        "status": "completed",
        "execution_time": f"{execution_time:.2f}s",
        "output_files": output_files,
        "metrics": {
            "lines_of_code": 150,
            "test_coverage": 85,
            "complexity_score": 3
        },
        "container": os.environ.get("HOSTNAME", "unknown"),
        "completed_at": datetime.now().isoformat()
    }


def execute_task_real(task: Dict[str, Any], requirements: str, output_dir: Path, executor_id: int) -> Dict[str, Any]:
    """Execute task using real Claude API"""
    # Import ExecutorAgent - assumes it's available in the container
    try:
        sys.path.insert(0, str(Path(__file__).parent))
        from executor_agent import ExecutorAgent

        task_output_dir = output_dir / f"executor_{executor_id}" / task["id"]
        task_output_dir.mkdir(parents=True, exist_ok=True)

        agent = ExecutorAgent(executor_id, task, str(task_output_dir))
        # Execute the specific task description, with overall requirements as context
        task_prompt = f"{task['description']}\n\nOverall project context: {requirements}"
        result = agent.execute(task_prompt)

        result["container"] = os.environ.get("HOSTNAME", "unknown")
        result["completed_at"] = datetime.now().isoformat()

        return result
    except ImportError as e:
        print(f"Warning: ExecutorAgent not available, falling back to simulated mode: {e}")
        return execute_task_simulated(task, output_dir, executor_id)


def main():
    """Main entry point for AWS Batch executor"""
    # Get configuration from environment variables
    task_id = os.environ.get("TASK_ID")
    s3_bucket = os.environ.get("S3_BUCKET")
    s3_prefix = os.environ.get("S3_PREFIX")
    region = os.environ.get("AWS_REGION", "eu-central-1")
    output_dir_str = os.environ.get("OUTPUT_DIR", "/output")
    use_real_str = os.environ.get("USE_REAL_EXECUTORS", "false")

    # Allow command-line overrides for testing
    parser = argparse.ArgumentParser(description='AWS Batch Task Executor')
    parser.add_argument('--task-id', default=task_id, help='Task ID to execute')
    parser.add_argument('--s3-bucket', default=s3_bucket, help='S3 bucket')
    parser.add_argument('--s3-prefix', default=s3_prefix, help='S3 prefix')
    parser.add_argument('--region', default=region, help='AWS region')
    parser.add_argument('--output-dir', default=output_dir_str, help='Output directory')
    parser.add_argument('--real', action='store_true', help='Use real executors')

    args = parser.parse_args()

    task_id = args.task_id
    s3_bucket = args.s3_bucket
    s3_prefix = args.s3_prefix
    region = args.region
    output_dir = Path(args.output_dir)
    use_real_executors = args.real or use_real_str.lower() == "true"

    # Validate required parameters
    if not task_id:
        print("ERROR: TASK_ID environment variable or --task-id argument required")
        sys.exit(1)

    if not s3_bucket or not s3_prefix:
        print("ERROR: S3_BUCKET and S3_PREFIX environment variables required")
        sys.exit(1)

    # Check Bedrock configuration
    use_bedrock = os.environ.get("CLAUDE_CODE_USE_BEDROCK", "0") == "1"

    print(f"AWS Batch Executor starting for task: {task_id}")
    print(f"  S3 bucket: {s3_bucket}")
    print(f"  S3 prefix: {s3_prefix}")
    print(f"  Region: {region}")
    print(f"  Output dir: {output_dir}")
    print(f"  Container: {os.environ.get('HOSTNAME', 'unknown')}")
    print(f"  Real executors: {use_real_executors}")
    print(f"  Using Bedrock: {use_bedrock}")

    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)

    # S3 paths
    s3_tasks_path = f"s3://{s3_bucket}/{s3_prefix}/tasks/{task_id}.json"
    s3_results_path = f"s3://{s3_bucket}/{s3_prefix}/results/{task_id}.json"

    # Download task definition from S3
    local_task_file = output_dir / f"task_{task_id}.json"
    print(f"  Downloading task definition from S3...")

    if not s3_download(s3_tasks_path, str(local_task_file), region):
        print(f"ERROR: Failed to download task definition from {s3_tasks_path}")
        sys.exit(1)

    # Load task definition
    try:
        with open(local_task_file, 'r') as f:
            task_def = json.load(f)
    except Exception as e:
        print(f"ERROR: Failed to parse task definition: {e}")
        sys.exit(1)

    task = task_def["task"]
    requirements = task_def.get("requirements", "")

    # Override use_real_executors from task definition if present
    if "use_real_executors" in task_def:
        use_real_executors = task_def["use_real_executors"]

    print(f"  Task: {task['name']}")
    print(f"  Description: {task.get('description', 'N/A')[:80]}")

    # Generate executor ID (use container hostname hash or random)
    executor_id = hash(os.environ.get("HOSTNAME", str(time.time()))) % 1000

    # Execute task
    try:
        if use_real_executors:
            print("  Executing with real Claude API...")
            result = execute_task_real(task, requirements, output_dir, executor_id)
        else:
            print("  Executing in simulated mode...")
            result = execute_task_simulated(task, output_dir, executor_id)

        print(f"  Task completed: {result['status']}")

    except Exception as e:
        print(f"ERROR: Task execution failed: {e}")

        result = {
            "task_id": task_id,
            "executor_id": executor_id,
            "task_name": task.get("name", "unknown"),
            "status": "failed",
            "error": str(e),
            "container": os.environ.get("HOSTNAME", "unknown"),
            "failed_at": datetime.now().isoformat()
        }

    # Write result locally
    local_result_file = output_dir / f"result_{task_id}.json"
    with open(local_result_file, 'w') as f:
        json.dump(result, f, indent=2)

    # Upload result to S3
    print(f"  Uploading result to S3...")
    if not s3_upload(str(local_result_file), s3_results_path, region):
        print(f"WARNING: Failed to upload result to {s3_results_path}")
        # Don't exit with error - the task itself succeeded

    # Upload generated files to S3
    if result.get("output_files"):
        print(f"  Uploading {len(result['output_files'])} generated files to S3...")
        s3_files_prefix = f"s3://{s3_bucket}/{s3_prefix}/files/{task_id}/"

        # Determine the task output directory to calculate clean relative paths
        task_output_dir = output_dir / f"executor_{executor_id}" / task_id

        for file_path in result["output_files"]:
            file_path_obj = Path(file_path)
            if file_path_obj.exists():
                # Get relative path from task output dir (cleaner paths)
                if file_path_obj.is_relative_to(task_output_dir):
                    rel_path = file_path_obj.relative_to(task_output_dir)
                else:
                    # Fallback to just the filename
                    rel_path = file_path_obj.name

                s3_file_path = f"{s3_files_prefix}{rel_path}"
                if s3_upload(file_path, s3_file_path, region):
                    print(f"    ✓ Uploaded {rel_path}")
                else:
                    print(f"    ✗ Failed to upload {rel_path}")

    # Exit with appropriate code
    if result.get("status") == "completed":
        print(f"Task {task_id} completed successfully")
        sys.exit(0)
    else:
        print(f"Task {task_id} failed: {result.get('error', 'Unknown error')}")
        sys.exit(1)


if __name__ == "__main__":
    main()
