#!/usr/bin/env python3
"""
SLURM Executor Script
Standalone script that runs on SLURM compute nodes to execute individual tasks
"""

import os
import sys
import json
import time
import fcntl
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional


def atomic_write_json(filepath: Path, data: Dict) -> None:
    """Write JSON file atomically using a temporary file"""
    temp_path = filepath.with_suffix('.tmp')
    with open(temp_path, 'w') as f:
        json.dump(data, f, indent=2)
        f.flush()
        os.fsync(f.fileno())
    temp_path.rename(filepath)


def read_json_with_lock(filepath: Path) -> Dict:
    """Read JSON file with file locking"""
    if not filepath.exists():
        return {}
    with open(filepath, 'r') as f:
        fcntl.flock(f.fileno(), fcntl.LOCK_SH)
        try:
            return json.load(f)
        finally:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)


def write_json_with_lock(filepath: Path, data: Dict) -> None:
    """Write JSON file with file locking"""
    mode = 'r+' if filepath.exists() else 'w'
    with open(filepath, mode) as f:
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
        try:
            f.seek(0)
            f.truncate()
            json.dump(data, f, indent=2)
        finally:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)


def update_task_status(state_dir: Path, task_id: str, status: str) -> None:
    """Update task status in state file with locking"""
    tasks_file = state_dir / "tasks.json"
    tasks_state = read_json_with_lock(tasks_file)

    # Remove from all status lists
    for status_list in ["pending", "in_progress", "completed", "failed"]:
        if task_id in tasks_state.get(status_list, []):
            tasks_state[status_list].remove(task_id)

    # Add to new status list
    if status not in tasks_state:
        tasks_state[status] = []
    if task_id not in tasks_state[status]:
        tasks_state[status].append(task_id)

    tasks_state["tasks"][task_id] = status

    write_json_with_lock(tasks_file, tasks_state)


def sync_to_s3(local_path: str, s3_path: str, region: str) -> bool:
    """Sync local path to S3"""
    try:
        if os.path.isdir(local_path):
            result = subprocess.run(
                ["aws", "s3", "sync", local_path, s3_path, "--region", region],
                capture_output=True,
                text=True,
                timeout=120
            )
        else:
            result = subprocess.run(
                ["aws", "s3", "cp", local_path, s3_path, "--region", region],
                capture_output=True,
                text=True,
                timeout=60
            )
        return result.returncode == 0
    except Exception as e:
        print(f"S3 sync failed: {e}")
        return False


def execute_task_simulated(task: Dict[str, Any], output_dir: Path, executor_id: int) -> Dict[str, Any]:
    """Execute task in simulated mode (for testing)"""
    task_output_dir = output_dir / f"executor_{executor_id}" / task["id"]
    task_output_dir.mkdir(parents=True, exist_ok=True)

    start_time = time.time()

    # Simulate work
    steps = 5
    for step in range(1, steps + 1):
        time.sleep(1)
        progress = (step / steps) * 100
        print(f"  Progress: {progress:.0f}% - Step {step}/{steps}")

    # Create simulated output files
    output_files = []
    for filename in ["implementation.py", "tests.py", "README.md"]:
        file_path = task_output_dir / filename
        with open(file_path, "w") as f:
            f.write(f"# {task['name']}\n\n")
            f.write(f"# Generated by SLURM Executor {executor_id}\n")
            f.write(f"# Task ID: {task['id']}\n")
            f.write(f"# Task: {task['description']}\n\n")
            f.write(f"# Implementation code would go here\n")
        output_files.append(str(file_path))

    execution_time = time.time() - start_time

    return {
        "task_id": task["id"],
        "executor_id": executor_id,
        "task_name": task["name"],
        "status": "completed",
        "execution_time": f"{execution_time:.2f}s",
        "output_files": output_files,
        "metrics": {
            "lines_of_code": 150,
            "test_coverage": 85,
            "complexity_score": 3
        },
        "node": os.environ.get("SLURMD_NODENAME", os.uname().nodename),
        "completed_at": datetime.now().isoformat()
    }


def execute_task_real(task: Dict[str, Any], requirements: str, output_dir: Path, executor_id: int) -> Dict[str, Any]:
    """Execute task using real Claude API"""
    # Import ExecutorAgent
    sys.path.insert(0, str(Path(__file__).parent))
    from executor_agent import ExecutorAgent

    task_output_dir = output_dir / f"executor_{executor_id}" / task["id"]
    task_output_dir.mkdir(parents=True, exist_ok=True)

    agent = ExecutorAgent(executor_id, task, str(task_output_dir))
    result = agent.execute(requirements)

    result["node"] = os.environ.get("SLURMD_NODENAME", os.uname().nodename)
    result["completed_at"] = datetime.now().isoformat()

    return result


def main():
    parser = argparse.ArgumentParser(description='SLURM Task Executor')
    parser.add_argument('--task-id', required=True, help='Task ID to execute')
    parser.add_argument('--state-dir', required=True, help='State directory path')
    parser.add_argument('--output-dir', required=True, help='Output directory path')

    # AWS mode options
    parser.add_argument('--aws-mode', action='store_true', help='Enable AWS mode with S3 sync')
    parser.add_argument('--s3-bucket', help='S3 bucket for AWS mode')
    parser.add_argument('--s3-prefix', help='S3 prefix for AWS mode')
    parser.add_argument('--aws-region', default='eu-central-1', help='AWS region')

    args = parser.parse_args()

    task_id = args.task_id
    state_dir = Path(args.state_dir)
    output_dir = Path(args.output_dir)

    print(f"SLURM Executor starting for task: {task_id}")
    print(f"  State dir: {state_dir}")
    print(f"  Output dir: {output_dir}")
    print(f"  Node: {os.environ.get('SLURMD_NODENAME', os.uname().nodename)}")
    print(f"  AWS mode: {args.aws_mode}")

    # Read task definition
    task_defs_dir = state_dir / "task_definitions"
    task_def_file = task_defs_dir / f"{task_id}.json"

    if not task_def_file.exists():
        print(f"ERROR: Task definition not found: {task_def_file}")
        sys.exit(1)

    task_def = read_json_with_lock(task_def_file)
    task = task_def["task"]
    requirements = task_def["requirements"]
    use_real_executors = task_def.get("use_real_executors", False)

    print(f"  Task: {task['name']}")
    print(f"  Real executors: {use_real_executors}")

    # Extract executor ID from job name or use default
    executor_id = int(os.environ.get("SLURM_ARRAY_TASK_ID", "1"))

    # Execute task
    try:
        if use_real_executors:
            print("  Executing with real Claude API...")
            result = execute_task_real(task, requirements, output_dir, executor_id)
        else:
            print("  Executing in simulated mode...")
            result = execute_task_simulated(task, output_dir, executor_id)

        print(f"  Task completed: {result['status']}")

        # Write result
        results_dir = state_dir / "results"
        results_dir.mkdir(parents=True, exist_ok=True)
        result_file = results_dir / f"{task_id}.json"
        atomic_write_json(result_file, result)

        # Update task status
        update_task_status(state_dir, task_id, "completed")

        # S3 sync if in AWS mode
        if args.aws_mode and args.s3_bucket:
            s3_results_path = f"s3://{args.s3_bucket}/{args.s3_prefix}/results/"
            s3_state_path = f"s3://{args.s3_bucket}/{args.s3_prefix}/state/"

            print("  Syncing to S3...")
            sync_to_s3(str(result_file), f"{s3_results_path}{task_id}.json", args.aws_region)
            sync_to_s3(str(state_dir / "tasks.json"), f"{s3_state_path}tasks.json", args.aws_region)

        print(f"Task {task_id} completed successfully")
        sys.exit(0)

    except Exception as e:
        print(f"ERROR: Task execution failed: {e}")

        # Write failure result
        error_result = {
            "task_id": task_id,
            "executor_id": executor_id,
            "task_name": task.get("name", "unknown"),
            "status": "failed",
            "error": str(e),
            "node": os.environ.get("SLURMD_NODENAME", os.uname().nodename),
            "failed_at": datetime.now().isoformat()
        }

        results_dir = state_dir / "results"
        results_dir.mkdir(parents=True, exist_ok=True)
        result_file = results_dir / f"{task_id}.json"
        atomic_write_json(result_file, error_result)

        # Update task status
        update_task_status(state_dir, task_id, "failed")

        # S3 sync if in AWS mode
        if args.aws_mode and args.s3_bucket:
            s3_results_path = f"s3://{args.s3_bucket}/{args.s3_prefix}/results/"
            sync_to_s3(str(result_file), f"{s3_results_path}{task_id}.json", args.aws_region)

        sys.exit(1)


if __name__ == "__main__":
    main()
